{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "PATH=Path('data/alica/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAS_PATH=Path('data/alica_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "LM_PATH=Path('data/alica_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['good', 'neutral', 'bad']\n",
    "d = {\"good\":2,\"neutral\":1,\"bad\":0}\n",
    "#d = {\"good\":1,\"neutral\":0}\n",
    "#d = {\"good\":1,\"bad\":0}\n",
    "col_names = ['labels','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"public.tsv\", sep = \"\\t\", header = None, quoting=csv.QUOTE_NONE)\n",
    "df.fillna(\"\", inplace=True)\n",
    "df[\"text\"] = df[1]+\"|\"+df[2]+\"|\"+df[3]+\"|\"+df[5]\n",
    "df[\"id\"] = df[0]\n",
    "df[\"num\"] = df[4]\n",
    "df[\"labels\"] = [0]*len(df)\n",
    "\n",
    "df.to_csv(CLAS_PATH/\"test.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"final.tsv\", sep = \"\\t\", header = None, quoting=csv.QUOTE_NONE)\n",
    "df.fillna(\"\", inplace=True)\n",
    "df[\"text\"] = df[1]+\"|\"+df[2]+\"|\"+df[3]+\"|\"+df[5]\n",
    "df[\"id\"] = df[0]\n",
    "df[\"num\"] = df[4]\n",
    "df[\"labels\"] = [0]*len(df)\n",
    "\n",
    "df.to_csv(CLAS_PATH/\"test_.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds neutral=bad\n",
    "df = pd.read_csv(PATH/\"train.tsv\", sep = \"\\t\", header = None, quoting=csv.QUOTE_NONE)\n",
    "df.head()\n",
    "\n",
    "df.fillna(\"\", inplace=True)\n",
    "df[\"text\"] = df[1]+\"|\"+df[2]+\"|\"+df[3]+\"|\"+df[5]\n",
    "df[\"id\"] = df[0]\n",
    "df[\"num\"] = df[4]\n",
    "df[\"labels\"] = df[6].apply(lambda x: d[x.replace(\"neutral\",\"bad\")])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "i = 0\n",
    "for train_index, valid_index in kf.split(df):\n",
    "    dff, dfv = df.iloc[train_index,:], df.iloc[valid_index,:]\n",
    "    \n",
    "    dff.to_csv(CLAS_PATH/f\"train_{i}.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    dfv.to_csv(CLAS_PATH/f\"valid_{i}.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    \n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds neutral=good\n",
    "df = pd.read_csv(PATH/\"train.tsv\", sep = \"\\t\", header = None, quoting=csv.QUOTE_NONE)\n",
    "df.head()\n",
    "\n",
    "df.fillna(\"\", inplace=True)\n",
    "df[\"text\"] = df[1]+\"|\"+df[2]+\"|\"+df[3]+\"|\"+df[5]\n",
    "df[\"id\"] = df[0]\n",
    "df[\"num\"] = df[4]\n",
    "df[\"labels\"] = df[6].apply(lambda x: d[x.replace(\"neutral\",\"good\")])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "i = 0\n",
    "for train_index, valid_index in kf.split(df):\n",
    "    dff, dfv = df.iloc[train_index,:], df.iloc[valid_index,:]\n",
    "    \n",
    "    dff.to_csv(CLAS_PATH/f\"train_{i}_.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    dfv.to_csv(CLAS_PATH/f\"valid_{i}_.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds 3\n",
    "df = pd.read_csv(PATH/\"train.tsv\", sep = \"\\t\", header = None, quoting=csv.QUOTE_NONE)\n",
    "df.head()\n",
    "\n",
    "df.fillna(\"\", inplace=True)\n",
    "df[\"text\"] = df[1]+\"|\"+df[2]+\"|\"+df[3]+\"|\"+df[5]\n",
    "df[\"id\"] = df[0]\n",
    "df[\"num\"] = df[4]\n",
    "df[\"labels\"] = df[6].apply(lambda x: d[x])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "i = 0\n",
    "for train_index, valid_index in kf.split(df):\n",
    "    dff, dfv = df.iloc[train_index,:], df.iloc[valid_index,:]\n",
    "    \n",
    "    dff.to_csv(CLAS_PATH/f\"train_{i}_3.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    dfv.to_csv(CLAS_PATH/f\"valid_{i}_3.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds no neutral\n",
    "df = pd.read_csv(PATH/\"train.tsv\", sep = \"\\t\", header = None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "df.fillna(\"\", inplace=True)\n",
    "df = df[df[6]!=\"neutral\"]\n",
    "\n",
    "df[\"text\"] = df[1]+\"|\"+df[2]+\"|\"+df[3]+\"|\"+df[5]\n",
    "df[\"id\"] = df[0]\n",
    "df[\"num\"] = df[4]\n",
    "df[\"labels\"] = df[6].apply(lambda x: d[x])\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "i = 0\n",
    "for train_index, valid_index in kf.split(df):\n",
    "    dff, dfv = df.iloc[train_index,:], df.iloc[valid_index,:]\n",
    "    \n",
    "    dff.to_csv(CLAS_PATH/f\"train_{i}_n.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    dfv.to_csv(CLAS_PATH/f\"valid_{i}_n.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds reg\n",
    "df = pd.read_csv(PATH/\"train.tsv\", sep = \"\\t\", header = None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "df.fillna(\"\", inplace=True)\n",
    "df = df[df[6]!=\"neutral\"]\n",
    "\n",
    "df[\"text\"] = df[1]+\"|\"+df[2]+\"|\"+df[3]+\"|\"+df[5]\n",
    "df[\"id\"] = df[0]\n",
    "df[\"num\"] = df[4]\n",
    "\n",
    "res = []\n",
    "for x,y in zip(df[6], df[7]):\n",
    "    if x == \"good\":\n",
    "        t = 1+y\n",
    "    if x == \"neutral\":\n",
    "        t = 1 \n",
    "    if x == \"bad\":\n",
    "        t = 1-y\n",
    "    res.append(t)\n",
    "\n",
    "df[\"labels\"] = res\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "i = 0\n",
    "for train_index, valid_index in kf.split(df):\n",
    "    dff, dfv = df.iloc[train_index,:], df.iloc[valid_index,:]\n",
    "    \n",
    "    dff.to_csv(CLAS_PATH/f\"train_{i}_r.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    dfv.to_csv(CLAS_PATH/f\"valid_{i}_r.csv\", columns=col_names+[\"id\",\"num\"], sep=\"\\t\", header=False, index=False)\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSubtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH/\"OpenSubtitles2016.en-ru.ru\", sep = \"\\t\", header = None, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[0]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts,val_texts = sklearn.model_selection.train_test_split(texts, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)}, columns=col_names)\n",
    "\n",
    "df_trn.to_csv(LM_PATH/'train.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=96000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace(u'\\xa0', u' ')\n",
    "    x = x.replace('|', '\\n')\n",
    "    x = x.replace('\\n\\n\\n', '\\n')\n",
    "    x = x.replace('\\n\\n', '\\n')\n",
    "    x = x.replace(\". . .\",\"...\")\n",
    "    x = \" , \".join(x.split(\",\"))\n",
    "    x = \" . \".join(x.split(\".\"))\n",
    "    x = x.replace(\".  .  .\",\"...\")\n",
    "    \n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df):\n",
    "    #labels = df[0].values.astype(np.int64)\n",
    "    labels = df[0].values.astype(float) #for reg\n",
    "    texts = '\\n' + df[1].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts), 'xx')\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(df):\n",
    "    tok, labels = [], []\n",
    "    for i, r in tqdm(enumerate(df)):\n",
    "        tok_, labels_ = get_texts(r)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(LM_PATH/'test.csv', header=None, chunksize=chunksize)\n",
    "tok_val, val_labels = get_all(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(LM_PATH/'train.csv', header=None, chunksize=chunksize)\n",
    "tok_trn, trn_labels = get_all(df_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "choice(tok_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH/'tmp').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'tok_trn_0.npy', tok_trn[:4000000])\n",
    "np.save(LM_PATH/'tmp'/'tok_trn_1.npy', tok_trn[4000000:8000000])\n",
    "np.save(LM_PATH/'tmp'/'tok_trn_2.npy', tok_trn[8000000:10000000])\n",
    "np.save(LM_PATH/'tmp'/'tok_trn_3.npy', tok_trn[10000000:12000000])\n",
    "np.save(LM_PATH/'tmp'/'tok_trn_4.npy', tok_trn[12000000:14000000])\n",
    "np.save(LM_PATH/'tmp'/'tok_trn_5.npy', tok_trn[14000000:15000000])\n",
    "np.save(LM_PATH/'tmp'/'tok_trn_6.npy', tok_trn[15000000:])\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.array([])\n",
    "for i in tqdm(range(7)):\n",
    "    tok_trn = np.append(tok_trn, np.load(LM_PATH/'tmp'/f'tok_trn_{i}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs=len(itos)\n",
    "vs,len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz,nh,nl = 400,1150,3\n",
    "\n",
    "wd=1e-7\n",
    "bptt=70\n",
    "bs=52\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.980186   3.859496   0.364186\n",
    "learner.save('lm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('lm_enc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm2')\n",
    "learner.save_encoder('lm_enc2')\n",
    "#3.961754   3.840744   0.365777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trn = pd.read_csv(CLAS_PATH/'train.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "#df_val = pd.read_csv(CLAS_PATH/'valid.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "#df_tst = pd.read_csv(CLAS_PATH/'test.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "df_tst = pd.read_csv(CLAS_PATH/'test_.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from random import choice\n",
    "#choice(tok_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok_trn, trn_labels = get_all(df_trn)\n",
    "#tok_val, val_labels = get_all(df_val)\n",
    "tok_tst, tst_labels = get_all(df_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "#np.save(CLAS_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "#np.save(CLAS_PATH/'tmp'/'tok_val.npy', tok_val)\n",
    "#np.save(CLAS_PATH/'tmp'/'tok_tst.npy', tok_tst)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_tst_.npy', tok_tst)\n",
    "\n",
    "#np.save(CLAS_PATH/'tmp'/'trn_labels.npy', trn_labels)\n",
    "#np.save(CLAS_PATH/'tmp'/'val_labels.npy', val_labels)\n",
    "#np.save(CLAS_PATH/'tmp'/'tst_labels.npy', tst_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'tst_labels_.npy', tst_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn.npy')\n",
    "#tok_val = np.load(CLAS_PATH/'tmp'/'tok_val.npy')\n",
    "#tok_tst = np.load(CLAS_PATH/'tmp'/'tok_tst.npy')\n",
    "tok_tst = np.load(CLAS_PATH/'tmp'/'tok_tst_.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "#val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "tst_clas = np.array([[stoi[o] for o in p] for p in tok_tst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(CLAS_PATH/'tmp'/'trn_ids.npy', trn_clas)\n",
    "#np.save(CLAS_PATH/'tmp'/'val_ids.npy', val_clas)\n",
    "#np.save(CLAS_PATH/'tmp'/'tst_ids.npy', tst_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'tst_ids_.npy', tst_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds nuetral=bad\n",
    "chunksize=96000\n",
    "\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    df_trn = pd.read_csv(CLAS_PATH/f'train_{i}.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df_val = pd.read_csv(CLAS_PATH/f'valid_{i}.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    tok_trn, trn_labels = get_all(df_trn)\n",
    "    tok_val, val_labels = get_all(df_val)\n",
    "    \n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_trn_{i}.npy', tok_trn)\n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_val_{i}.npy', tok_val)\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_labels_{i}.npy', trn_labels)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_labels_{i}.npy', val_labels)\n",
    "    \n",
    "    trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "    val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_ids_{i}.npy', trn_clas)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_ids_{i}.npy', val_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds nuetral=good\n",
    "chunksize=96000\n",
    "\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    df_trn = pd.read_csv(CLAS_PATH/f'train_{i}_.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df_val = pd.read_csv(CLAS_PATH/f'valid_{i}_.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    tok_trn, trn_labels = get_all(df_trn)\n",
    "    tok_val, val_labels = get_all(df_val)\n",
    "    \n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_trn_{i}_.npy', tok_trn)\n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_val_{i}_.npy', tok_val)\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_labels_{i}_.npy', trn_labels)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_labels_{i}_.npy', val_labels)\n",
    "    \n",
    "    trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "    val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_ids_{i}_.npy', trn_clas)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_ids_{i}_.npy', val_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds 3\n",
    "chunksize=96000\n",
    "\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    df_trn = pd.read_csv(CLAS_PATH/f'train_{i}_3.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df_val = pd.read_csv(CLAS_PATH/f'valid_{i}_3.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    tok_trn, trn_labels = get_all(df_trn)\n",
    "    tok_val, val_labels = get_all(df_val)\n",
    "    \n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_trn_{i}_3.npy', tok_trn)\n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_val_{i}_3.npy', tok_val)\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_labels_{i}_3.npy', trn_labels)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_labels_{i}_3.npy', val_labels)\n",
    "    \n",
    "    trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "    val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_ids_{i}_3.npy', trn_clas)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_ids_{i}_3.npy', val_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds no neutral\n",
    "chunksize=96000\n",
    "\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    df_trn = pd.read_csv(CLAS_PATH/f'train_{i}_n.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df_val = pd.read_csv(CLAS_PATH/f'valid_{i}_n.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    tok_trn, trn_labels = get_all(df_trn)\n",
    "    tok_val, val_labels = get_all(df_val)\n",
    "    \n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_trn_{i}_n.npy', tok_trn)\n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_val_{i}_n.npy', tok_val)\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_labels_{i}_n.npy', trn_labels)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_labels_{i}_n.npy', val_labels)\n",
    "    \n",
    "    trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "    val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_ids_{i}_n.npy', trn_clas)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_ids_{i}_n.npy', val_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds reg\n",
    "chunksize=96000\n",
    "\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    df_trn = pd.read_csv(CLAS_PATH/f'train_{i}_r.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df_val = pd.read_csv(CLAS_PATH/f'valid_{i}_r.csv', header=None, chunksize=chunksize, sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    tok_trn, trn_labels = get_all(df_trn)\n",
    "    tok_val, val_labels = get_all(df_val)\n",
    "    \n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_trn_{i}_r.npy', tok_trn)\n",
    "    np.save(CLAS_PATH/'tmp'/f'tok_val_{i}_r.npy', tok_val)\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_labels_{i}_r.npy', trn_labels)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_labels_{i}_r.npy', val_labels)\n",
    "    \n",
    "    trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "    val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "    np.save(CLAS_PATH/'tmp'/f'trn_ids_{i}_r.npy', trn_clas)\n",
    "    np.save(CLAS_PATH/'tmp'/f'val_ids_{i}_r.npy', val_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/6676114C76111E7D/Kaggle/Alica/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds neutral=bad\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48\n",
    "c = 2\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.75\n",
    "\n",
    "lr=1e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "tst_clas = np.load(CLAS_PATH/'tmp'/'tst_ids_.npy')\n",
    "tst_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'tst_labels_.npy'))\n",
    "tst_ds = TextDataset(tst_clas, tst_labels)\n",
    "tst_samp = SimpleSampler(tst_clas)\n",
    "tst_dl = DataLoader(tst_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=tst_samp)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    trn_clas = np.load(CLAS_PATH/'tmp'/f'trn_ids_{i}.npy')\n",
    "    val_clas = np.load(CLAS_PATH/'tmp'/f'val_ids_{i}.npy')\n",
    "    \n",
    "    trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'trn_labels_{i}.npy'))\n",
    "    val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'val_labels_{i}.npy'))\n",
    "\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "    \n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    \n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    \n",
    "    md = ModelData(PATH, trn_dl, val_dl, tst_dl)\n",
    "\n",
    "    m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=25.\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "#     wd = 0\n",
    "#     learn.load_encoder('lm_enc1')\n",
    "#     learn.freeze_to(-1)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}')\n",
    "\n",
    "#     learn.freeze_to(-2)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}')\n",
    "    \n",
    "#     wd = 1e-7\n",
    "#     learn.unfreeze()\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}')\n",
    "    \n",
    "#     learn.fit(lrs, 2, wds=wd, cycle_len=1)\n",
    "#     learn.save(f'fold_{i}')\n",
    "    learn.load(f'./10_folds_fwd_bad_86283/fold_{i}')\n",
    "    \n",
    "    wd = 1e-7\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(10,10,0.95,0.85))\n",
    "    learn.save(path+f'fold_{i}')\n",
    "\n",
    "    preds = learn.predict(is_test=True)\n",
    "    \n",
    "    df = pd.read_csv(CLAS_PATH/'test_.csv', header=None, sep = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df[\"prob\"] = [c[1] for c in np.exp(preds)] \n",
    "    df.sort_values(by=[2,\"prob\"], inplace=True, ascending=[True,False])\n",
    "    df.to_csv(f\"./blend1/_fold_{i}.tsv\",columns=[2,3,\"prob\"],index=False,sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds neutral=good\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48\n",
    "c = 2\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.75\n",
    "\n",
    "lr=1e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "tst_clas = np.load(CLAS_PATH/'tmp'/'tst_ids_.npy')\n",
    "tst_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'tst_labels_.npy'))\n",
    "tst_ds = TextDataset(tst_clas, tst_labels)\n",
    "tst_samp = SimpleSampler(tst_clas)\n",
    "tst_dl = DataLoader(tst_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=tst_samp)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    trn_clas = np.load(CLAS_PATH/'tmp'/f'trn_ids_{i}_.npy')\n",
    "    val_clas = np.load(CLAS_PATH/'tmp'/f'val_ids_{i}_.npy')\n",
    "    \n",
    "    trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'trn_labels_{i}_.npy'))\n",
    "    val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'val_labels_{i}_.npy'))\n",
    "\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "    \n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    \n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    \n",
    "    md = ModelData(PATH, trn_dl, val_dl, tst_dl)\n",
    "\n",
    "    m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=25.\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "#     wd = 0\n",
    "#     learn.load_encoder('lm_enc1')\n",
    "#     learn.freeze_to(-1)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_')\n",
    "\n",
    "#     learn.freeze_to(-2)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_')\n",
    "    \n",
    "#     wd = 1e-7\n",
    "#     learn.unfreeze()\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_')\n",
    "    \n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1)\n",
    "#     learn.save(f'fold_{i}_')\n",
    "    learn.load(f'./10_folds_fwd_good_85739/fold_{i}_')\n",
    "    \n",
    "    wd = 1e-7\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(10,10,0.95,0.85))\n",
    "    learn.save(path+f'fold_{i}_')\n",
    "        \n",
    "    preds = learn.predict(is_test=True)\n",
    "    \n",
    "    df = pd.read_csv(CLAS_PATH/'test_.csv', header=None, sep = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df[\"prob\"] = [c[1] for c in np.exp(preds)] \n",
    "    df.sort_values(by=[2,\"prob\"], inplace=True, ascending=[True,False])\n",
    "    df.to_csv(f\"./blend1/_fold_{i}_.tsv\",columns=[2,3,\"prob\"],index=False,sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds 3\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48\n",
    "c = 3\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.75\n",
    "\n",
    "lr=1e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "tst_clas = np.load(CLAS_PATH/'tmp'/'tst_ids_.npy')\n",
    "tst_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'tst_labels_.npy'))\n",
    "tst_ds = TextDataset(tst_clas, tst_labels)\n",
    "tst_samp = SimpleSampler(tst_clas)\n",
    "tst_dl = DataLoader(tst_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=tst_samp)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    trn_clas = np.load(CLAS_PATH/'tmp'/f'trn_ids_{i}_3.npy')\n",
    "    val_clas = np.load(CLAS_PATH/'tmp'/f'val_ids_{i}_3.npy')\n",
    "    \n",
    "    trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'trn_labels_{i}_3.npy'))\n",
    "    val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'val_labels_{i}_3.npy'))\n",
    "\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "    \n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    \n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    \n",
    "    md = ModelData(PATH, trn_dl, val_dl, tst_dl)\n",
    "\n",
    "    m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=25.\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "#     wd = 0\n",
    "#     learn.load_encoder('lm_enc1')\n",
    "#     learn.freeze_to(-1)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_3')\n",
    "\n",
    "#     learn.freeze_to(-2)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_3')\n",
    "    \n",
    "#     wd = 1e-7\n",
    "#     learn.unfreeze()\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_3')\n",
    "    \n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1)\n",
    "#     learn.save(f'fold_{i}_3')\n",
    "    \n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1)\n",
    "#     learn.save(f'fold_{i}_3')\n",
    "    learn.load(f'./10_folds_fwd_3_1.4_86962/fold_{i}_3')\n",
    "    \n",
    "    wd = 1e-7\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(10,10,0.95,0.85))\n",
    "    learn.save(path+f'fold_{i}_3')\n",
    "    \n",
    "    preds = learn.predict(is_test=True)\n",
    "    \n",
    "    df = pd.read_csv(CLAS_PATH/'test_.csv', header=None, sep = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df[\"prob1\"] = [c[1] for c in np.exp(preds)]\n",
    "    df[\"prob2\"] = [c[2] for c in np.exp(preds)]\n",
    "    df.to_csv(f\"./blend1/_fold_{i}_3.tsv\",columns=[2,3,\"prob1\",\"prob2\"],index=False,sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds neutral\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48\n",
    "c = 2\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.75\n",
    "\n",
    "lr=1e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "tst_clas = np.load(CLAS_PATH/'tmp'/'tst_ids_.npy')\n",
    "tst_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'tst_labels_.npy'))\n",
    "tst_ds = TextDataset(tst_clas, tst_labels)\n",
    "tst_samp = SimpleSampler(tst_clas)\n",
    "tst_dl = DataLoader(tst_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=tst_samp)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    trn_clas = np.load(CLAS_PATH/'tmp'/f'trn_ids_{i}_n.npy')\n",
    "    val_clas = np.load(CLAS_PATH/'tmp'/f'val_ids_{i}_n.npy')\n",
    "    \n",
    "    trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'trn_labels_{i}_n.npy'))\n",
    "    val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'val_labels_{i}_n.npy'))\n",
    "\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "    \n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    \n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    \n",
    "    md = ModelData(PATH, trn_dl, val_dl, tst_dl)\n",
    "\n",
    "    m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=25.\n",
    "    learn.metrics = [accuracy]\n",
    "\n",
    "#     wd = 0\n",
    "#     learn.load_encoder('lm_enc1')\n",
    "#     learn.freeze_to(-1)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_n')\n",
    "\n",
    "#     learn.freeze_to(-2)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_n')\n",
    "    \n",
    "#     wd = 1e-7\n",
    "#     learn.unfreeze()\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_n')\n",
    "    \n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1)\n",
    "#     learn.save(f'fold_{i}_n')\n",
    "    learn.load(f'./10_folds_fwd_neutral/fold_{i}_n')\n",
    "    \n",
    "    wd = 1e-7\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(10,10,0.95,0.85))\n",
    "    learn.save(path+f'fold_{i}_n')\n",
    "    \n",
    "    preds = learn.predict(is_test=True)\n",
    "    \n",
    "    df = pd.read_csv(CLAS_PATH/'test_.csv', header=None, sep = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df[\"prob\"] = [c[1] for c in np.exp(preds)]\n",
    "    df.sort_values(by=[2,\"prob\"], inplace=True, ascending=[True,False])\n",
    "    df.to_csv(f\"./blend1/_fold_{i}_n.tsv\",columns=[2,3,\"prob\"],index=False,sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folds reg\n",
    "itos = pickle.load((LM_PATH/'tmp'/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48\n",
    "c = 1\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.75\n",
    "\n",
    "lr=1e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])\n",
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])\n",
    "\n",
    "tst_clas = np.load(CLAS_PATH/'tmp'/'tst_ids_.npy')\n",
    "tst_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'tst_labels_.npy'))\n",
    "tst_ds = TextDataset(tst_clas, tst_labels)\n",
    "tst_samp = SimpleSampler(tst_clas)\n",
    "tst_dl = DataLoader(tst_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=tst_samp)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    trn_clas = np.load(CLAS_PATH/'tmp'/f'trn_ids_{i}_r.npy')\n",
    "    val_clas = np.load(CLAS_PATH/'tmp'/f'val_ids_{i}_r.npy')\n",
    "    \n",
    "    trn_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'trn_labels_{i}_r.npy'))\n",
    "    val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/f'val_labels_{i}_r.npy'))\n",
    "\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "    \n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    \n",
    "    trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "    \n",
    "    md = ModelData(PATH, trn_dl, val_dl, tst_dl)\n",
    "\n",
    "    m = get_rnn_regression(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "              layers=[em_sz, 50, c], drops=[dps[4], 0.1],\n",
    "              dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n",
    "\n",
    "    learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "    learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "    learn.clip=25.\n",
    "    \n",
    "    learn.crit = F.mse_loss\n",
    "\n",
    "#     wd = 0\n",
    "#     learn.load_encoder('lm_enc1')\n",
    "#     learn.freeze_to(-1)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_r')\n",
    "\n",
    "#     learn.freeze_to(-2)\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_r')\n",
    "    \n",
    "#     wd = 1e-7\n",
    "#     learn.unfreeze()\n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "#     learn.save(f'fold_{i}_r')\n",
    "    \n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1)\n",
    "#     learn.save(f'fold_{i}_r')\n",
    "    \n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1)\n",
    "#     learn.save(f'fold_{i}_r')\n",
    "    \n",
    "#     learn.fit(lrs, 1, wds=wd, cycle_len=1)\n",
    "#     learn.save(f'fold_{i}_r')\n",
    "    learn.load(f'./10_folds_fwd_mse_86488/fold_{i}_r')\n",
    "    \n",
    "    wd = 1e-7\n",
    "    learn.unfreeze()\n",
    "    learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(10,10,0.95,0.85))\n",
    "    learn.save(path+f'fold_{i}_r')\n",
    "    \n",
    "    preds = learn.predict(is_test=True)\n",
    "    \n",
    "    df = pd.read_csv(CLAS_PATH/'test_.csv', header=None, sep = \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    df[\"prob\"] = [c[0] for c in preds]\n",
    "    df.to_csv(f\"./blend1/fold_{i}_r.tsv\",columns=[2,3,\"prob\"],index=False,sep=\"\\t\",header=False)"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "gist": {
   "data": {
    "description": "fastai.text imdb example",
    "public": true
   },
   "id": "0dd0df21cf404cf2bb51d0148c8b7d8b"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
